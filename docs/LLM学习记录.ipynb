{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024/11/18\n",
    "\n",
    "* 学到 self-attention 和 multi-head-attention 的机制\n",
    "\n",
    "### 浏览博客时了解到：\n",
    "<br>**目前训练超大规模语言模型技术路线：GPU + PyTorch + Megatron-LM + DeepSpeed**\n",
    "<br><br>\n",
    "*一 为什么需要Deepspeed\n",
    "分布式计算环境中，主节点负责协调其他节点和进程的工作<br>\n",
    "<br>pytorch官方提供的分布式训练工具Accelerate只支持nvlink，而T4，3090这类显卡是PIX ，检测方式：nvidia-smi topo -m；deepspeed支持更大规模的模型训练<br>\n",
    "<br>**混合精度训练**\n",
    "ZeRO可以减少内存占用，优化大模型训练，将模型参数分成了三个部分：Optimizer States、Gradient 和 Model Parameter。在使用 ZeRO 进行分布式训练时，可以选 ZeRO-Offload 和 ZeRO-Stage3 等不同的优化技术。<br>\n",
    "<br>\n",
    "大模型（LLM）在训练时往往需要大量内存来存储中间激活、权重等参数，百亿模型甚至无法在单个 GPU上进行训练，使得模型训练在某些情况下非常低效和不可能。这就需要进行多卡，或者多节点分布式训练。<br>\n",
    "<br>在大规模深度学习模型训练中有个主要范式：\n",
    "<br>(1) 数据并行\n",
    "<br>(2) 模型并行<br>\n",
    "<br>————————————————<br>\n",
    "<br>原文链接：https://blog.csdn.net/zwqjoy/article/details/130732601*\n",
    "\n",
    "### 以下今天看到的是对理解 LLM 技术有用的文章\n",
    "1. 《Attention is All You Need》浅读（简介+代码）By 苏剑林 \n",
    "https://kexue.fm/archives/4765\n",
    "\n",
    "1. LLM入门指南 - 密排六方橘子的文章 - 知乎 （建立认知概念）\n",
    "https://zhuanlan.zhihu.com/p/669193585\n",
    "\n",
    "2. 想学习大语言模型(LLM)，应该从哪个开源模型开始？ - 三风的回答 - 知乎（介绍学习路线）\n",
    "https://www.zhihu.com/question/608820310/answer/3267334734\n",
    "\n",
    "### 看到句挺好的话\n",
    "多读英文，多接触横向项目，关注应用技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024/11/17\n",
    "今天配置了用于深度学习的环境，具体见\n",
    "[配置记录.md](./配置记录.md)\n",
    "\n",
    "此外，收集了一份 [Linuc目录详细说明.md](./Linux目录详细说明.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_vtuber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
